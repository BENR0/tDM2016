---
title: "Temporal Datamining WS 2"
author: "Benjamin Roesner, Marta Lemanczyk, Jiachun Zhang"
output:
  html_document:
    fig_caption: yes
    fig_height: 5
    fig_width: 7
    highlight: pygments
    number_sections: yes
    theme: united
    toc: yes
    toc_depth: 2
  pdf_document:
    toc: yes
---
------

```{r chunk options and functions, echo=FALSE, eval = TRUE}
#eval: evaluate code chunk true/false
#include: include chunk in ouput document true/false (code still evaluated and plots generated)
#echo: include source code true/
#error: true/false display error messages
#message: true/ false display messages
#warning: true/false display warning messages
#eval: true/false evaluate block -> use with variable eval_block
#cache: cache results -> use with variable cache_erg
#fig.cap = figNum("FIG CAPTION"): figure captions, with numbering by figNum function (see below)
#anchor = "figure": use with package kfigr to get inline reference to images with number by using function figr("NAME OF CHUNK WITH FIGURE")

#child = "file.Rmd": include file

#links
#[SECTION NAME][LINK NAME]

#citation from lit.bib
#[@LITSHORTNAME]

#figure number counter function
figNum = local({
  i = 0
  function(x) {
    i <-  i + 1
    paste("Figure ", i, ": ", x, sep = "")
  }
})

#flag to easily change evaluation of code blocks in results section
eval_block <- FALSE
#flag if reading and plotting should be cached
cache_erg <- FALSE
```

```{r problem specific functions, echo = FALSE, include = FALSE}
loadData <- function(flist, names = unlist(lapply(seq(1, length(flist)), function(x) paste0("d", x)))){
  for (f in files){
    assign(names[which(flist == f)], as.data.frame(ReadLRN(basename(f), dirname(f))$Data), envir = .GlobalEnv)
    }
}

dataSizeZeros <- function(dataCol){
  len <- length(dataCol)
  lenZ <- length(which(dataCol == 0))
  res <- list(NumDataPoints = len, NumMissing = lenZ)
  return(res)
}

# Funktion, um die erste bzw.letzte 10 Ertraege zu sehen
# Funktion, um die erste bzw.letzte 10 Ertraege zu sehen
printData <- function(data){
  print("Erste bzw. letzte 10 Werte des Datensatzes")
  head(data, n = 10)
  tail(data, n = 10)
}

# Histogram,PDEplot,QQplot,Boxplot,NaN Anzahl
overviewPlot <- function(data, name = "data"){
  par(mfrow = c(2,3)) 
  #mat <- matrix(c(1,2,0,3,4,5),2,byrow = T)
  #layout(mat,c(5,5,1),c(1,1))
  
  MinD <- nanmin(data)
  MaxD <- nanmax(data)
  
  # Histogramm
  hist(data, ylab = "Nr of Data")
  
  # PDE-Dichte
  #PDEplot(data,title=name, xlab=name);  # Bei mir kann das Plot nicht verkleinert werden
  pdeVal <- ParetoDensityEstimation(data, ParetoRadius(data))
  plot(pdeVal$kernels, pdeVal$paretoDensity, type = "l", xlab = "Data", ylab = "PDE", main = name)
  #plot(density(data),main=name);
  
  plot.new()
  
  # QQplot
  qqnorm(data, ylab = name)
  gridOn() 
  
  # Boxplot
  boxplot(data, xlab=name, main = paste("Range:[ ", num2str(round(MinD, 5)), " ,", num2str(round(MaxD, 5)) ," ]"), axes = FALSE)
  
  
  # Barplot fuer die NaNs
  NaNs <- (sum(is.infinite(data)) + sum(is.na(data)))/length(data)
  barplot(NaNs, ylab = "NaNs in %", main = paste(round(NaNs, 4), " %"), xlim = c(0, 3), ylim = c(0, 1))
  if (any(is.nan(data), na.rm = TRUE)) 
    print("NaNs in Data found. This message is only important, if after rounding the percent of NaN is zero in the bar plot.")
  if (any(is.infinite(data), na.rm = TRUE)) 
    warning("Infinite values in Data found.")
  
  par(mfrow = c(1,1))
}
```

```{r load_packages, echo=FALSE, error=FALSE, message=FALSE, include=FALSE}
#load packages
packages <- c("tidyr", "dplyr", "DataIO", "lubridate", "zoo", "stringr", "timeSeries", "tseries", "car", "AdaptGauss", "forecast", "kfigr", "dbt.RetroMAT", "RHmm", "FinTS")
lapply(packages,library,character.only=TRUE)
#set prefix for figure numbering
#opts_knit$set(kfigr.prefix = TRUE)

```


```{r load_data, echo = FALSE, eval = eval_block}
files <- list.files("../data", pattern = "p2_", full.names = TRUE)

loadData(files, c("dataRad", "dataNO3", "dataPrecip"))

dataNO3$HH <- str_pad(dataNO3$HH, 2, c("left"), "0")
dataNO3$Min <- str_pad(dataNO3$Min, 2, c("left"), "0")

dataNO3 <- unite_(dataNO3, "time", c("HH", "Min"), sep = ":")
dataNO3 <- unite_(dataNO3, "date", c("YYYY", "MM", "DD"), sep = "-")
dataNO3 <- unite_(dataNO3, "datetime", c("date", "time" ), sep = " ")
dataNO3 <- select(dataNO3, datetime, NNO3mgl)
dataNO3$datetime <- parse_date_time(dataNO3$datetime, order = "Ymd HM")


dataRad$HH <- str_pad(dataRad$HH, 2, c("left"), "0")
dataRad$Min <- str_pad(dataRad$Min, 2, c("left"), "0")

dataRad <- unite_(dataRad, "time", c("HH", "Min"), sep = ":")
dataRad <- unite_(dataRad, "date", c("YYYY", "MM", "DD"), sep = "-")
dataRad <- unite_(dataRad, "datetime", c("date", "time" ), sep = " ")
dataRad <- select(dataRad, datetime, CorrectedNetRadiationWm2)
dataRad$datetime <- parse_date_time(dataRad$datetime, order = "Ymd HM")


dataPrecip$HH <- str_pad(dataPrecip$HH, 2, c("left"), "0")
dataPrecip$Min <- str_pad(dataPrecip$Min, 2, c("left"), "0")

dataPrecip <- unite_(dataPrecip, "time", c("HH", "Min"), sep = ":")
dataPrecip <- unite_(dataPrecip, "date", c("YYYY", "MM", "DD"), sep = "-")
dataPrecip <- unite_(dataPrecip, "datetime", c("date", "time" ), sep = " ")
dataPrecip <- select(dataPrecip, datetime, PrecipitationMM)
dataPrecip$datetime <- parse_date_time(dataPrecip$datetime, order = "Ymd HM")

```
#### Überblick über die Variablen


```{r inspect_data_length, echo = FALSE, eval = eval_block}
#size of data sets and how many missing values
missNO3 <- dataSizeZeros(dataNO3$NNO3mgl)
missPrecip <- dataSizeZeros(dataPrecip$PrecipitationMM)
missRad <- dataSizeZeros(dataRad$CorrectedNetRadiationWm2)

#range of data points
summary(dataNO3)
summary(dataPrecip)
summary(dataRad)
```

```{r inspect_data_plot, echo = FALSE, fig.cap = "Inspect data", anchor = "figure", eval = eval_block}
#get visual impression of data
plot(dataNO3$NNO3mgl, pch = "l")
plot(dataPrecip$PrecipitationMM,pch = "l")
plot(dataRad$CorrectedNetRadiationWm2,pch = "l")
```


```{r inspect_data_hist, echo = FALSE, eval = eval_block}
hist(dataNO3$NNO3mgl)
hist(dataPrecip$PrecipitationMM)
hist(dataRad$CorrectedNetRadiationWm2)
#check if data ist normal distributed
qqPlot(data$NNO3mgl)
qqPlot(dataPrecip$PrecipitationMM)
qqPlot(dataRad$CorrectedNetRadiationWm2)
```


```{r load, echo = FALSE}
###############################################################################################
#Aufgabe 1
#Inspizieren sie die einzelnen Zeitreihen mit den Methoden der Knowledge Discovery
############################################################################################
FileName <- c("p2_Chirimachay_NO3","p2_Chirimachay_Netradiation.lrn","p2_Chirimachay_Precip.lrn")

## Pfad bitte hier anpassen!!!
Directory  <- "../data/"


NO3 <- ReadLRN(FileName[1],Directory)
Netra <- ReadLRN(FileName[2],Directory)
Prec <- ReadLRN(FileName[3],Directory)

############################################################################
#NO3
attach(NO3)
```

#### Inspizieren sie die einzelnen Zeitreihen mit den Methoden der Knowledge Discovery
Zur Untersuchung sind Datensätze „Chirimachay_NO3.lrn“ , „Chirimachay_Netradiation.lrn“ und „Chirimachay_Precip.lrn“.
Die Datensätze sind jeweils Liste mit 7 Variablen, so dass die Messungswerte und Messungszeiten genau notiert werden.
Zuerst untersuchen wir den Datensatz „Chirimachay_NO3.lrn“.
Die erste Erträge sowie die letzten Erträge der Variable NO3 sehen wie folgt aus:
```{r inspect vars, echo = FALSE}
# Messungsfrequenz
printData(NO3$Data)
```

Es sind insgesamt 22695 Werte jede 30 Minuten von 10 Uhr, 14.5.2014 bis 5 Uhr, 30.8.2015 gemessen worden.
Die Zeitreihe verläuft steitg, vorbei einige Fehlstellen durch Intepolation ersetzt werden kann,
da es gibt Sprungen zwischen Zeitpunkten zu beobachten hat. Die Zeitreihe ist nicht stationär, auch
hetroskedastisch, daher ist die auch kein weißes Rauschen. Für die Verteilung der zeitunabhängige
Beobachtungswerte „NNO3mgl“ wirde folgende Histogramm, PDE plot, QQ-plot gegen Normalverteilung,
Boxplot sowie das Barplot für NaNs ausgegeben.

```{r inspect vars 1, echo = FALSE}
# Ueberblick von Zeitreihe
year <- Data[,3]
indy <- which(year == 2015, arr.ind = T)

plot(Data[, 1], type="l", ylab = "NO3", xlab = "Zeit", col = "blue")
abline(v = min(indy), col = c("red"),  lwd = 3)

# Verteilungsbetrachtung
#library("reshape2")
#library("caTools")
#par(mfrow=c(2,3))
#InspectVariable(Data[,1],Header[1])  # Muss zuerst Pakete importieren. 
#par(mfrow=c(1,1))                    #Bei mir kann die Graphiken nicht innerhalb ein Bild legen. Version von R?

overviewPlot(Data[, 1], Header[1])
```

 - Keine NaNs, Es gibt ganz vielen Nullen. → NaNs durch Nullen eingesetzt.
 - Wertbereich sehr klein.
 - Nicht normalverteilt. 

Versuche jetzt die extrem kleine Werte (< 0,05) wegzunehmen
```{r inspect var ohne null, echo = FALSE}
# Eliminierung der Nullen und extrem kleinen Werten 
ohneNull <- Data[, 1][Data[, 1] != 0]

overviewPlot(ohneNull, paste(Header[1], "ohne Null"))
```

 - nach Eliminierung der Nullen ist es deutlich, dass es sich um einen Wachstum handelt. -> logaritmieren?
 - Wertbereich sehr klein → prozentuierung

Nach mehrfach Durfühung der Box-cox Transformation hat Faktor 0,5 als Empfehlung erhalten:
```{r inspect var trafo, echo = FALSE, eval = eval_block}
# Transformieren Daten, um Normalverteilung zu annaehren
#### aus welchem Paket ist boxcoxTrans???
boxcoxTrans(ohneNull[sample(1:length(ohneNull), 5000)])
```

Betrachte daher die Transformation (Wurzel(Daten)*100):
```{r inspect var trafo 2, echo = FALSE}
#trans <- slog(ohneNull)*100
#trans <- slog(1/ohneNull)
trans <- (sqrt(ohneNull) * 100)
overviewPlot(trans, paste("Transformierte", Header[1]))
```

 - 	Die Approximation in der Mitte sehr gut, bei kleiner Werte ist Approximation sehr schlecht.
    Muss erneut angepasst werden.
 -  Gaussian Mixture Modell


#### GMM
```{r GMM, echo = FALSE, eval = eval_block}
temp <- AdaptGauss(trans)

Werte1 <- list(Means = c(7.029219, 17.786148, 29.106267), 
           SDs = c(5.08932, 4.50063, 5.69700), 
           Weights = c( 0.176, 0.348, 0.490))
res1 <- AdaptGauss(trans, Werte1$Means, Werte1$SDs, Werte1$Weights)
dput(res1)
ab <- Bayes4Mixtures(trans, Werte1$Means, Werte1$SDs, Werte1$Weights, PlotIt = T) 
```

Hier sieht man, dass die Anpassung bei der extrem Werte Bereich immer noch schlecht ist.
Betrachte noch die statistische Verifizierung.

```{r gmm verification, echo = FALSE}
Werte1 <- list(Means = c(7.029219, 17.786148, 29.106267), 
           SDs = c(5.08932, 4.50063, 5.69700), 
           Weights = c( 0.176, 0.348, 0.490))
#Verifizierung
QQplotGMM(trans, Werte1$Means, Werte1$SDs, Werte1$Weights)

abc <- Chi2testMixtures(trans, Werte1$Means, Werte1$SDs, Werte1$Weights, PlotIt = T)
```

P-Wert sehr klein. H0 wrid abgeleht. Anpassung schlecht.


##### Clusterung der Daten
Die Beobachtung kann zu vier Klassen geteilt werden, Niedrige Stoffkonzentration,
mittele Stoffkonzentration hoch Stoffkonzentration sowie die Nullen.
```{r clusterung, echo = FALSE}
#Clusterung
dec <- BayesDecisionBoundaries(Werte1$Means, Werte1$SDs, Werte1$Weights)
Cls <- ClassifyByDecisionBoundaries(trans, dec)

ind <- which(Data[, 1] != 0)
ind2 <- which(Data[, 1] == 0)

plot(ind, Data[ind, 1], ylab = "NO3", xlab = "Zeit", col = Cls, main = "Clusterung", ylim = c(-0, 0.4))
points(ind2, Data[ind2, 1], col = "blue")
abline(v = min(indy), col = c("pink"), lwd = 3)
legend("topright", c("Hoch", "Mittel", "Niedrig", "Null"), col = c("green", "red", "black", "blue"), pch = 1)


detach(NO3)

#############################################################################
#Netradiation
printData(Netra$Data)

##############################################################################
#Precip
printData(Prec$Data)
```


#### Komponentmodell
```{r Komponentenmodell, echo = FALSE, eval = eval_block}
###################################################################################
#Aufgabe 2
#Versuchen Sie jede der Variablen mit einem passenden Ansatz zu modellieren. 
#Orientieren sie sich hierbei an der Vorlesungseinheit zur Hydrologie.
########################################################################################
#Residualanalysis
showTrend <- function(data,trend,trendName){
  index <- 1:length(data)
  par(mfrow = c(2,2))
  
  plot(data, type="l", col="blue", main = trendName)
  lines(trend$fitted.value, col = "green")
  
  res <- trend$residuals
  plot(res, type = "l", col = "red", xlab = "Zeit", ylab = "Residual", main = paste("Residuals, EW=", round(mean(res), 4)))
  abline(h = 0)
  
  qqnorm(res, ylab = "Residuals", main = "Normal QQplot")
  gridOn()
  
  plot(res[-1], res[-length(data)], main = paste("Autokorrelation: ", round(cor(res[-1], res[-length(data)]), 4)), xlab = "X_t", ylab = "X_t+1")
  abline(0, 1, col = "blue", lwd = 3)
 
  
  par(mfrow = c(1, 1))
}

# Wahl des Polynomgrades
polynomGrad <- function(data){
  index <- 1:length(data)
  trend1 <- lm(data ~ index)
  fit1.SSE <- sum(resid(trend1)^2)
  for (i in 2:10){
    trend.poly <- lm(data ~ poly(index, i))
    sse <- sum(resid(trend.poly)^2)
    fit1.SSE <- c(fit1.SSE, sse)
  }
  plot(fit1.SSE, type = "l", xlab = "Grad", ylab = "SSE", main = "Screen Kriterium")
}


## Hier data einsetzen
polynomGrad(data)

#OptimalTrend: 3th Polynom
trend <- lm(data ~ poly(index, 3))
showTrend(data, trend3, "3th Polynom")
```


#### ARMA/ARIMA
```{r ARIMA, echo = FALSE}
# Step0. Pr?fe zuerst die Stationarit?t
#plot(data,type="l",col="blue");
#title(paste("Erwartungswert: ", mean(data)))
      
data <- NO3$Data[,1]

Differenz <- diff(data)
plot(Differenz, type = "l", col = "green")
title(paste("Differenz, Erwartungswert: ", mean(Differenz)))


# Step1. Modell Postulat : Zeichne die ACF und PACF
acfpacfPlot <- function(sample){
  par(mfrow = c(1, 2))
  
  acf(sample)
  pacf(sample)
  
  par(mfrow = c(1, 1))
}

acfpacfPlot(Differenz)

# Step2. Model Parameter Bestimmung
fit <- Arima(Differenz, order = c(0, 0, 3), include.mean = F)
fit$coef

# Step3. Modell Validierung
# one-step forecast
plot(Differenz, type = "l", col = "blue")
points(fitted(fit), col = "red", type = "l")
res2 <- Differenz - fitted(fit)

# Residuum aktuelle Modell
plot(Differenz, type = "l", col = "blue")
points(fit$residuals, col = "red", type = "l")
res <- Differenz - fit$residuals

#Residuumanalysis
par(mfrow = c(2,3))
plot(res2, xlab="time", ylab = "residuals", main = "MA(3) Residuals with one-step forecast")
plot(res2[-length(res2)], res2[-1], main = "Autokorelation") 
qqnorm(res2, ylab = "MA(3) Residuals with one-step forecast")
gridOn()
 
plot(res, xlab = "time", ylab = "residuals", main = "MA(3) Residuals with Model fitting")
plot(res[-length(res)], res[-1], main = "Autokorelation")
qqnorm(res, ylab = "MA(3) Residuals with Model fitting")
gridOn()

par(mfrow = c(1, 1))
```


#### ARCH/GARCH

Differenzreihe betrachten.
ARCH LM-Test, teste, ob ARCH Effekte gibt:
```{r Arch/ Garch, echo = FALSE, include = TRUE}
#Beste AR Modell Schatzen, Berechne die Regression der Yt-1 auf die Yt (KQ) um die Fehler e_t zu erhalten
#Berechne die KQ Regression von Fehler
#Teste die gegenseitige Signifikanz der Parameter
#Falls einige dieser Koeffizienten signifikant konstruiere ein entsprechendes ARCH Modell

acfpacfPlot(Differenz)
#acfpacfPlot(Differenz^2)

ArchTest(Differenz)
```

Nullhypothese wird abgelehnt, es gibt ARCH Effekt.
Die Parameter wird hier durch Anprobieren bestimmt, dann durch AIC Kriterium das beste Parameter bestimmt.
Also hier wird GARCH(1,1) ausgewählt.
```{r Arch/ Garch 2, echo = FALSE, message = FALSE, include = FALSE}
gar1 <- garch(Differenz,order = c(1,0))
gar2 <- garch(Differenz,order = c(2,0))
gar3 <- garch(Differenz,order = c(3,0))
gar4 <- garch(Differenz,order = c(1,1))
gar5 <- garch(Differenz,order = c(1,2))
gar6 <- garch(Differenz,order = c(2,1))
gar7 <- garch(Differenz,order = c(2,2))
aic <- c()
aic <- c(aic,AIC(gar1))
aic <- c(aic,AIC(gar2))
aic <- c(aic,AIC(gar3))
aic <- c(aic,AIC(gar4))
aic <- c(aic,AIC(gar5))
aic <- c(aic,AIC(gar6))
aic <- c(aic,AIC(gar7))
```

```{r Arch/ Garch plot, echo = FALSE}
plot(aic, type = "l")
points(aic)
legend("topright", paste(1:7, ": GARCH(",c("1,0","2,0","3,0","1,1","1,2","2,1","2,2"),")"), pch = 1)
```

Jarque Bera Test ist ein statistischer Test, der anhand der Schiefe und der Kurtosis in den Daten prüft,
ob eine Normalverteilung vorliegt. Hier zeigt, dass die Residuen keine Normalverteilung ist.
Nach des Skripts muss Residuen Chi2 verteilt sein.
```{r Arch/ Garch 3, echo = FALSE, include = FALSE}
summary(gar4)
#Jarque Bera Test: test, ob Residuals normalverteilt ist.
#Residual soll Chi2 verteilt ist --> heavy tail
#Box-Ljung test: test,ob Autokorrelation gibt.
#Residual soll kein Autokorrelation vorliegen

overviewPlot(gar4$residuals, "Residuen für GARCH(1,1)")

plot(gar4)
```

Die PDEplot sieht gut aus.
Box-Ljung Test testet auf der Autokorelation, hier zeigt keine Autokorelation innerhalb der Residuen.
Laut des Skripts sollen Risiduen keine Autokorrelation unterliegenm, daher stellt das GARCH(1,1) 
eine gute Anpassung dar.


#### Hidden Markov Modell

NO3 ohne Nullen hat 3 Gaussian Mixture, s.o, nutze daher Transformierte Daten und 3 Bestände.
```{r Hidden Markow Modell, echo = FALSE, message = FALSE, include = FALSE}
#NO3 ohne Nullen hat 3 Gaussian Mixture.
HMM <- HMMFit(trans, nStates = 3)
HMM$HMM$initProb
HMM$HMM$transMat
HMM$HMM$distribution
summary(HMM)

vit <- viterbi(HMM, trans)
#HMMGraphicDiag(vit, HMM, trans)
```

```{r Hidden Markow Modell plot, echo = FALSE}
par(mfrow = c(3, 1))
plot(vit$states, type = "l", main = "Clusterung nach Viterbi")
plot(Cls, type = "l", main = "Clusterung nach GMM")
plot(vit$states - Cls, type = "l", main = "Diffenz von Clusterung")
#par(mfrow = c(1, 1))
```

HMM liefert ganz anderes Ergebnis als GMM.


#### Fast Foriour Transformation


#### Aufgabe 3
Beschreiben Sie den Wissensgewinn durch diese Modelle. Wie und wodurch wird NO3 beeinflusst?
