```{r load, echo = FALSE}
###############################################################################################
#Aufgabe 1
#Inspizieren sie die einzelnen Zeitreihen mit den Methoden der Knowledge Discovery
############################################################################################
FileName <- c("p2_Chirimachay_NO3","p2_Chirimachay_Netradiation.lrn","p2_Chirimachay_Precip.lrn")

## Pfad bitte hier anpassen!!!
Directory  <- "../data/"


NO3 <- ReadLRN(FileName[1],Directory)
Netra <- ReadLRN(FileName[2],Directory)
Prec <- ReadLRN(FileName[3],Directory)

############################################################################
#NO3
attach(NO3)
```

#### Inspizieren sie die einzelnen Zeitreihen mit den Methoden der Knowledge Discovery
Zur Untersuchung sind Datensätze „Chirimachay_NO3.lrn“ , „Chirimachay_Netradiation.lrn“ und „Chirimachay_Precip.lrn“.
Die Datensätze sind jeweils Liste mit 7 Variablen, so dass die Messungswerte und Messungszeiten genau notiert werden.
Zuerst untersuchen wir den Datensatz „Chirimachay_NO3.lrn“.
Die erste Erträge sowie die letzten Erträge der Variable NO3 sehen wie folgt aus:
```{r inspect vars, echo = FALSE}
# Messungsfrequenz
printData(NO3$Data)
```

Es sind insgesamt 22695 Werte jede 30 Minuten von 10 Uhr, 14.5.2014 bis 5 Uhr, 30.8.2015 gemessen worden.
Die Zeitreihe verläuft steitg, vorbei einige Fehlstellen durch Intepolation ersetzt werden kann,
da es gibt Sprungen zwischen Zeitpunkten zu beobachten hat. Die Zeitreihe ist nicht stationär, auch
hetroskedastisch, daher ist die auch kein weißes Rauschen. Für die Verteilung der zeitunabhängige
Beobachtungswerte „NNO3mgl“ wirde folgende Histogramm, PDE plot, QQ-plot gegen Normalverteilung,
Boxplot sowie das Barplot für NaNs ausgegeben.

```{r inspect vars 1, echo = FALSE}
# Ueberblick von Zeitreihe
year <- Data[,3]
indy <- which(year == 2015, arr.ind = T)

plot(Data[, 1], type="l", ylab = "NO3", xlab = "Zeit", col = "blue")
abline(v = min(indy), col = c("red"),  lwd = 3)

# Verteilungsbetrachtung
#library("reshape2")
#library("caTools")
#par(mfrow=c(2,3))
#InspectVariable(Data[,1],Header[1])  # Muss zuerst Pakete importieren. 
#par(mfrow=c(1,1))                    #Bei mir kann die Graphiken nicht innerhalb ein Bild legen. Version von R?

overviewPlot(Data[, 1], Header[1])
```

 - Keine NaNs, Es gibt ganz vielen Nullen. → NaNs durch Nullen eingesetzt.
 - Wertbereich sehr klein.
 - Nicht normalverteilt. 

Versuche jetzt die extrem kleine Werte (< 0,05) wegzunehmen
```{r inspect var ohne null, echo = FALSE}
# Eliminierung der Nullen und extrem kleinen Werten 
ohneNull <- Data[, 1][Data[, 1] != 0]

overviewPlot(ohneNull, paste(Header[1], "ohne Null"))
```

 - nach Eliminierung der Nullen ist es deutlich, dass es sich um einen Wachstum handelt. -> logaritmieren?
 - Wertbereich sehr klein → prozentuierung

Nach mehrfach Durfühung der Box-cox Transformation hat Faktor 0,5 als Empfehlung erhalten:
```{r inspect var trafo, echo = FALSE, eval = eval_block}
# Transformieren Daten, um Normalverteilung zu annaehren
#### aus welchem Paket ist boxcoxTrans???
boxcoxTrans(ohneNull[sample(1:length(ohneNull), 5000)])
```

Betrachte daher die Transformation (Wurzel(Daten)*100):
```{r inspect var trafo 2, echo = FALSE}
#trans <- slog(ohneNull)*100
#trans <- slog(1/ohneNull)
trans <- (sqrt(ohneNull) * 100)
overviewPlot(trans, paste("Transformierte", Header[1]))
```

 - 	Die Approximation in der Mitte sehr gut, bei kleiner Werte ist Approximation sehr schlecht.
    Muss erneut angepasst werden.
 -  Gaussian Mixture Modell


#### GMM
```{r GMM, echo = FALSE, eval = eval_block}
temp <- AdaptGauss(trans)

Werte1 <- list(Means = c(7.029219, 17.786148, 29.106267), 
           SDs = c(5.08932, 4.50063, 5.69700), 
           Weights = c( 0.176, 0.348, 0.490))
res1 <- AdaptGauss(trans, Werte1$Means, Werte1$SDs, Werte1$Weights)
dput(res1)
ab <- Bayes4Mixtures(trans, Werte1$Means, Werte1$SDs, Werte1$Weights, PlotIt = T) 
```

Hier sieht man, dass die Anpassung bei der extrem Werte Bereich immer noch schlecht ist.
Betrachte noch die statistische Verifizierung.

```{r gmm verification, echo = FALSE}
Werte1 <- list(Means = c(7.029219, 17.786148, 29.106267), 
           SDs = c(5.08932, 4.50063, 5.69700), 
           Weights = c( 0.176, 0.348, 0.490))
#Verifizierung
QQplotGMM(trans, Werte1$Means, Werte1$SDs, Werte1$Weights)

abc <- Chi2testMixtures(trans, Werte1$Means, Werte1$SDs, Werte1$Weights, PlotIt = T)
```

P-Wert sehr klein. H0 wrid abgeleht. Anpassung schlecht.


##### Clusterung der Daten
Die Beobachtung kann zu vier Klassen geteilt werden, Niedrige Stoffkonzentration,
mittele Stoffkonzentration hoch Stoffkonzentration sowie die Nullen.
```{r clusterung, echo = FALSE}
#Clusterung
dec <- BayesDecisionBoundaries(Werte1$Means, Werte1$SDs, Werte1$Weights)
Cls <- ClassifyByDecisionBoundaries(trans, dec)

ind <- which(Data[, 1] != 0)
ind2 <- which(Data[, 1] == 0)

plot(ind, Data[ind, 1], ylab = "NO3", xlab = "Zeit", col = Cls, main = "Clusterung", ylim = c(-0, 0.4))
points(ind2, Data[ind2, 1], col = "blue")
abline(v = min(indy), col = c("pink"), lwd = 3)
legend("topright", c("Hoch", "Mittel", "Niedrig", "Null"), col = c("green", "red", "black", "blue"), pch = 1)


detach(NO3)

#############################################################################
#Netradiation
printData(Netra$Data)

##############################################################################
#Precip
printData(Prec$Data)
```


#### Komponentmodell
```{r Komponentenmodell, echo = FALSE, eval = eval_block}
###################################################################################
#Aufgabe 2
#Versuchen Sie jede der Variablen mit einem passenden Ansatz zu modellieren. 
#Orientieren sie sich hierbei an der Vorlesungseinheit zur Hydrologie.
########################################################################################
#Residualanalysis
showTrend <- function(data,trend,trendName){
  index <- 1:length(data)
  par(mfrow = c(2,2))
  
  plot(data, type="l", col="blue", main = trendName)
  lines(trend$fitted.value, col = "green")
  
  res <- trend$residuals
  plot(res, type = "l", col = "red", xlab = "Zeit", ylab = "Residual", main = paste("Residuals, EW=", round(mean(res), 4)))
  abline(h = 0)
  
  qqnorm(res, ylab = "Residuals", main = "Normal QQplot")
  gridOn()
  
  plot(res[-1], res[-length(data)], main = paste("Autokorrelation: ", round(cor(res[-1], res[-length(data)]), 4)), xlab = "X_t", ylab = "X_t+1")
  abline(0, 1, col = "blue", lwd = 3)
 
  
  par(mfrow = c(1, 1))
}

# Wahl des Polynomgrades
polynomGrad <- function(data){
  index <- 1:length(data)
  trend1 <- lm(data ~ index)
  fit1.SSE <- sum(resid(trend1)^2)
  for (i in 2:10){
    trend.poly <- lm(data ~ poly(index, i))
    sse <- sum(resid(trend.poly)^2)
    fit1.SSE <- c(fit1.SSE, sse)
  }
  plot(fit1.SSE, type = "l", xlab = "Grad", ylab = "SSE", main = "Screen Kriterium")
}


## Hier data einsetzen
polynomGrad(data)

#OptimalTrend: 3th Polynom
trend <- lm(data ~ poly(index, 3))
showTrend(data, trend3, "3th Polynom")
```


#### ARMA/ARIMA
```{r ARIMA, echo = FALSE}
# Step0. Pr?fe zuerst die Stationarit?t
#plot(data,type="l",col="blue");
#title(paste("Erwartungswert: ", mean(data)))
      
data <- NO3$Data[,1]

Differenz <- diff(data)
plot(Differenz, type = "l", col = "green")
title(paste("Differenz, Erwartungswert: ", mean(Differenz)))


# Step1. Modell Postulat : Zeichne die ACF und PACF
acfpacfPlot <- function(sample){
  par(mfrow = c(1, 2))
  
  acf(sample)
  pacf(sample)
  
  par(mfrow = c(1, 1))
}

acfpacfPlot(Differenz)

# Step2. Model Parameter Bestimmung
fit <- Arima(Differenz, order = c(0, 0, 3), include.mean = F)
fit$coef

# Step3. Modell Validierung
# one-step forecast
plot(Differenz, type = "l", col = "blue")
points(fitted(fit), col = "red", type = "l")
res2 <- Differenz - fitted(fit)

# Residuum aktuelle Modell
plot(Differenz, type = "l", col = "blue")
points(fit$residuals, col = "red", type = "l")
res <- Differenz - fit$residuals

#Residuumanalysis
par(mfrow = c(2,3))
plot(res2, xlab="time", ylab = "residuals", main = "MA(3) Residuals with one-step forecast")
plot(res2[-length(res2)], res2[-1], main = "Autokorelation") 
qqnorm(res2, ylab = "MA(3) Residuals with one-step forecast")
gridOn()
 
plot(res, xlab = "time", ylab = "residuals", main = "MA(3) Residuals with Model fitting")
plot(res[-length(res)], res[-1], main = "Autokorelation")
qqnorm(res, ylab = "MA(3) Residuals with Model fitting")
gridOn()

par(mfrow = c(1, 1))
```


#### ARCH/GARCH

Differenzreihe betrachten.
ARCH LM-Test, teste, ob ARCH Effekte gibt:
```{r Arch/ Garch, echo = FALSE, include = TRUE}
#Beste AR Modell Schatzen, Berechne die Regression der Yt-1 auf die Yt (KQ) um die Fehler e_t zu erhalten
#Berechne die KQ Regression von Fehler
#Teste die gegenseitige Signifikanz der Parameter
#Falls einige dieser Koeffizienten signifikant konstruiere ein entsprechendes ARCH Modell

acfpacfPlot(Differenz)
#acfpacfPlot(Differenz^2)

ArchTest(Differenz)
```

Nullhypothese wird abgelehnt, es gibt ARCH Effekt.
Die Parameter wird hier durch Anprobieren bestimmt, dann durch AIC Kriterium das beste Parameter bestimmt.
Also hier wird GARCH(1,1) ausgewählt.
```{r Arch/ Garch 2, echo = FALSE, message = FALSE, include = FALSE}
gar1 <- garch(Differenz,order = c(1,0))
gar2 <- garch(Differenz,order = c(2,0))
gar3 <- garch(Differenz,order = c(3,0))
gar4 <- garch(Differenz,order = c(1,1))
gar5 <- garch(Differenz,order = c(1,2))
gar6 <- garch(Differenz,order = c(2,1))
gar7 <- garch(Differenz,order = c(2,2))
aic <- c()
aic <- c(aic,AIC(gar1))
aic <- c(aic,AIC(gar2))
aic <- c(aic,AIC(gar3))
aic <- c(aic,AIC(gar4))
aic <- c(aic,AIC(gar5))
aic <- c(aic,AIC(gar6))
aic <- c(aic,AIC(gar7))
```

```{r Arch/ Garch plot, echo = FALSE}
plot(aic, type = "l")
points(aic)
legend("topright", paste(1:7, ": GARCH(",c("1,0","2,0","3,0","1,1","1,2","2,1","2,2"),")"), pch = 1)
```

Jarque Bera Test ist ein statistischer Test, der anhand der Schiefe und der Kurtosis in den Daten prüft,
ob eine Normalverteilung vorliegt. Hier zeigt, dass die Residuen keine Normalverteilung ist.
Nach des Skripts muss Residuen Chi2 verteilt sein.
```{r Arch/ Garch 3, echo = FALSE, include = FALSE}
summary(gar4)
#Jarque Bera Test: test, ob Residuals normalverteilt ist.
#Residual soll Chi2 verteilt ist --> heavy tail
#Box-Ljung test: test,ob Autokorrelation gibt.
#Residual soll kein Autokorrelation vorliegen

overviewPlot(gar4$residuals, "Residuen für GARCH(1,1)")

plot(gar4)
```

Die PDEplot sieht gut aus.
Box-Ljung Test testet auf der Autokorelation, hier zeigt keine Autokorelation innerhalb der Residuen.
Laut des Skripts sollen Risiduen keine Autokorrelation unterliegenm, daher stellt das GARCH(1,1) 
eine gute Anpassung dar.


#### Hidden Markov Modell

NO3 ohne Nullen hat 3 Gaussian Mixture, s.o, nutze daher Transformierte Daten und 3 Bestände.
```{r Hidden Markow Modell, echo = FALSE, message = FALSE, include = FALSE}
#NO3 ohne Nullen hat 3 Gaussian Mixture.
HMM=HMMFit(trans, nStates=3,control=list(verbose=1, init= 'KMEANS'),asymptCov=TRUE) # K-means initialisierung
HMM$HMM$initProb
HMM$HMM$transMat
HMM$HMM$distribution
summary(HMM)

ZustandsNamen=c('Z1','Z2','Z3')
PlotPixMatrix(HMM$HMM$transMat,ZustandsNamen,0,1,ZustandsNamen,main = 'geschätzte Übergangswahrschinlichkeiten')
#PlotPixMatrix(HMMmodell$HMM$initProb,LowLim = 0,HiLim = 1,main = 'geschätzte Anfangswahrscheinlichkeiten')


VitPath=viterbi(HMM,trans)
HMMcls <- VitPath$states  # die vorhergesagten Klassen des HMM
HMMGraphicDiag(vit,HMM,trans)
```

```{r Hidden Markow Modell plot, echo = FALSE}
par(mfrow=c(3,1))
plot(HMMcls,type="l",main="Clusterung nach Viterbi")
plot(Cls,type="l",main="Clusterung nach GMM")
#Cls=ReadCLS('PriceVolumeHMMstate.cls',path)$Cls
#plot(Cls,type="l",main="Vordefinierte Clusterung")
plot(vit$states-Cls,type="l",main="Differenz von Clusterung")
par(mfrow=c(1,1))
```

HMM liefert ganz anderes Ergebnis als GMM.


#### Fast Foriour Transformation
```{r Hidden Markow Modell, echo = FALSE, message = FALSE, include = FALSE}
data=NO3$Data[,1]
plot(data,type="l")

#Step 1: Symmetrie und Spiegeln, wenn nötig
periodData=c(data,rev(data),-data,-rev(data))
plot(periodData,type="l")
data=periodData

#Step 2: FFT, Amplitude plotten
KomplexeFourierKoeffizienten=fft(data)
KomplexeFourierKoeffizienten[1:20]

Amplitude = sqrt(Re(KomplexeFourierKoeffizienten * Conj(KomplexeFourierKoeffizienten)))
#power=Amplitude^2
plot(Amplitude[1:100],type="h")

NyquistNumber=length(data)/2
MeanigfulFreqInd = 1:NyquistNumber
freq = MeanigfulFreqInd /length(data)
plot(freq,Amplitude[MeanigfulFreqInd],type="l",xlab='Frequenz',ylab="log Amplitude")


#Step 3: Filterung
# Hin und Ruecktransformieren
FilteredFreq = KomplexeFourierKoeffizienten
gefilterd = Re(ifft(FilteredFreq))
plot(data,type="l",col='blue',lwd=3)
lines(gefilterd,col='green',lwd=1)
legend("topright",c('original','fft then ifft'),col=c('blue','green'),lty = 1,lwd=c(3,1))


#Filterung durch weglassen der HF anteile
KeepInd = (1:40);  # nur die tieffrequenten behalten 
KeepInd = c(1:20,80:100); # Ausgewählt Frequenz
m=median(Amplitude[1:100]) # Signifikant Frequenz
keepInd = which(Amplitude[1:100]> m)
FilteredFreq = KomplexeFourierKoeffizienten*0;
FilteredFreq[KeepInd]= KomplexeFourierKoeffizienten[KeepInd];
gefilterd1 = Re(ifft(FilteredFreq));
plot(data,type="l",col='blue',lwd=1)
lines(gefilterd1,col='green',lwd=1)
legend("topright",c('original','chosen freq'),col=c('blue','green'),lty = 1,lwd=c(3,1))
```


#### Aufgabe 3
Beschreiben Sie den Wissensgewinn durch diese Modelle. Wie und wodurch wird NO3 beeinflusst?
